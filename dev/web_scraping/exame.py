import requests
from bs4 import BeautifulSoup
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import RSLPStemmer
from nltk.stem import WordNetLemmatizer
import string, re
from urllib.parse import urljoin
from datetime import timedelta, datetime


# ---- Utilit√°rios ----
def converter_nome_empresa_para_url(nome_empresa):
    """
    Converte o nome da empresa para o formato usado pela Exame em URLs.
    Exemplo: 'Banco do Brasil' -> 'banco-do-brasil'
    - Normaliza para min√∫sculas
    - Remove acentos
    - Troca espa√ßos por h√≠fens
    - Remove caracteres especiais
    """
    nome_lower = nome_empresa.lower()
    nome_limpo = re.sub(r'[√†√°√¢√£√§√•]', 'a', nome_lower)
    nome_limpo = re.sub(r'[√®√©√™√´]', 'e', nome_limpo)
    nome_limpo = re.sub(r'[√¨√≠√Æ√Ø]', 'i', nome_limpo)
    nome_limpo = re.sub(r'[√≤√≥√¥√µ√∂]', 'o', nome_limpo)
    nome_limpo = re.sub(r'[√π√∫√ª√º]', 'u', nome_limpo)
    nome_limpo = re.sub(r'[√ß]', 'c', nome_limpo)
    nome_url = re.sub(r'\s+', '-', nome_limpo)  # espa√ßos -> h√≠fens
    nome_url = re.sub(r'[^a-z0-9\-]', '', nome_url)  # remove s√≠mbolos
    return nome_url


# ---- NLTK ----
# Download silencioso de pacotes necess√°rios do NLTK
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('rslp', quiet=True)
nltk.download('punkt_tab', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

# Stopwords e processadores lingu√≠sticos
stop_words = set(stopwords.words('portuguese'))
stemmer = RSLPStemmer()
lemmatizer = WordNetLemmatizer()


def processar(texto):
    """
    Pipeline b√°sico de PLN:
    - Limpeza de pontua√ß√£o
    - Tokeniza√ß√£o
    - Remo√ß√£o de stopwords
    - Stemming (radical)
    - Lemmatiza√ß√£o (forma can√¥nica)
    Retorna dicion√°rio com tokens, stems e lemmas.
    """
    texto = texto.lower()
    texto = texto.translate(str.maketrans("", "", string.punctuation))
    texto = re.sub(r'[^a-z√°√©√≠√≥√∫√¢√™√Æ√¥√ª√£√µ√ß ]', ' ', texto)
    tokens = word_tokenize(texto, language='portuguese')
    tokens = [t for t in tokens if t not in stop_words and len(t) > 2]
    stems = [stemmer.stem(t) for t in tokens]
    lemmas = [lemmatizer.lemmatize(t) for t in tokens]
    return {"tokens": tokens, "stems": stems, "lemmas": lemmas}


# ---- Scraping ----
headers = {"User-Agent": "Mozilla/5.0"}  # Header para evitar bloqueios no request


# --- Conversor de tempo relativo (ex: "h√° 2 dias") ---
def parse_relative_date(texto):
    """
    Converte express√µes como 'h√° 2 dias', 'h√° 5 horas' em timedelta.
    Retorna None se n√£o reconhecer o padr√£o.
    """
    texto = texto.lower()
    match = re.search(r"h√° (\d+) (\w+)", texto)
    if not match:
        return None
    num = int(match.group(1))
    unidade = match.group(2)
    if "dia" in unidade:
        return timedelta(days=num)
    elif "hora" in unidade:
        return timedelta(hours=num)
    elif "min" in unidade:
        return timedelta(minutes=num)
    else:
        return None


# --- Conversor de data em portugu√™s ---
def parse_portuguese_date(date_text):
    """
    Converte datas escritas em portugu√™s para datetime.
    Exemplo: '10 de setembro de 2025 √†s 10h55'
    Retorna None se n√£o conseguir converter.
    """
    if not date_text or date_text == "Sem data":
        return None

    meses = {
        'janeiro': 1, 'fevereiro': 2, 'mar√ßo': 3, 'abril': 4,
        'maio': 5, 'junho': 6, 'julho': 7, 'agosto': 8,
        'setembro': 9, 'outubro': 10, 'novembro': 11, 'dezembro': 12
    }

    try:
        date_text = re.sub(r'^Publicado em\s*', '', date_text, flags=re.IGNORECASE)
        date_text = date_text.strip()
        # Corrige espa√ßamentos quebrados pela Exame
        date_text = re.sub(r'√†s(\d)', r'√†s \1', date_text)
        date_text = re.sub(r'(\d)h(\d)', r'\1h\2', date_text)

        pattern = r'(\d+)\s+de\s+(\w+)\s+de\s+(\d{4})\s+√†s\s+(\d{1,2})h(\d{2})'
        match = re.search(pattern, date_text, re.IGNORECASE)

        if match:
            dia = int(match.group(1))
            mes_nome = match.group(2).lower()
            ano = int(match.group(3))
            hora = int(match.group(4))
            minuto = int(match.group(5))

            if mes_nome in meses:
                mes = meses[mes_nome]
                return datetime(ano, mes, dia, hora, minuto)

        return None
    except Exception as e:
        print(f"Erro ao converter data '{date_text}': {e}")
        return None


# --- Extrator de links de artigos com filtro de tempo ---
def get_article_links_period(page_url, base_url, dias_max=30):
    """
    Busca links e t√≠tulos de artigos de uma p√°gina da Exame.
    Filtra para manter apenas os publicados nos √∫ltimos X dias (padr√£o=7).
    """
    resp = requests.get(page_url, headers=headers)
    soup = BeautifulSoup(resp.text, "html.parser")

    links = []
    for card in soup.select("h3 a.touch-area[href]"):
        href = urljoin(base_url, card["href"])
        titulo = card.get_text(strip=True)

        # Busca informa√ß√£o de tempo (h√° X dias/horas)
        parent_div = card.find_parent("div")
        tempo_tag = parent_div.select_one("div p.title-small")
        if tempo_tag:
            delta = parse_relative_date(tempo_tag.get_text(strip=True))
            if delta is None or delta > timedelta(days=dias_max):
                continue  # ignora not√≠cias antigas

        links.append((titulo, href))
    return links


# --- Extrator de conte√∫do de um artigo ---
def get_article_content(url):
    """
    Coleta os principais elementos de um artigo:
    - T√≠tulo
    - Manchete (headline)
    - Autor
    - Data de publica√ß√£o
    - Corpo da not√≠cia (texto limpo)
    """
    r = requests.get(url, headers=headers)
    sp = BeautifulSoup(r.text, "html.parser")

    # T√≠tulo principal (√†s vezes indexado no [1])
    titulo_tag = sp.select("h1")[1]
    titulo = titulo_tag.get_text(strip=True) if titulo_tag else "Sem t√≠tulo"

    # Manchete (subt√≠tulo)
    manchete_tag = sp.select_one("h2.title-medium")
    manchete = manchete_tag.get_text(strip=True) if manchete_tag else titulo

    # Autor (links que come√ßam com /autor/)
    autor_tag = sp.select_one('a[href^="/autor/"]')
    autor = autor_tag.get_text(strip=True) if autor_tag else None

    # Data de publica√ß√£o
    data_tag = sp.select_one("#news-component > div:nth-child(2) > p")
    if data_tag:
        data_pub_text = data_tag.get_text(strip=True)
        data_pub_text = re.sub(r'\s+', ' ', data_pub_text)
        data_pub_text = re.sub(r'<!--\s*-->', '', data_pub_text)
        data_pub_text = data_pub_text.strip()
        data_pub = parse_portuguese_date(data_pub_text) or data_pub_text
    else:
        data_pub = None

    # Corpo da not√≠cia
    corpo_div = sp.find("div", id="news-body")
    texto = ""
    if corpo_div:
        for ad in corpo_div.find_all(id=re.compile(r"^ads_|^banner_")):
            ad.decompose()  # remove an√∫ncios
        for tabela in corpo_div.find_all("table"):
            tabela.decompose()  # remove tabelas
        paragrafos = corpo_div.find_all("p")
        texto = " ".join(p.get_text(" ", strip=True) for p in paragrafos)

    return {
        "titulo": titulo,
        "manchete": manchete,
        "autor": autor,
        "data": data_pub,
        "texto": texto,
        "link": url
    }


# --- Fun√ß√£o principal ---
def web_scrapping(trading_mode, ticker, ticker_id, company_name, news_service=None):
    """
    Executa o scraping na Exame para uma empresa espec√≠fica.
    - Monta URL da empresa
    - Percorre pagina√ß√£o coletando artigos recentes
    - Extrai conte√∫do e processa com NLP (tokens, stems, lemmas)
    - Salva no banco (se news_service for fornecido) ou em CSV
    """
    company_url = converter_nome_empresa_para_url(company_name)
    base_url = f"https://exame.com/noticias-sobre/{company_url}/"

    print(f"Executando web scraping Exame para: {company_name}")
    print(f"URL base: {base_url}")

    dados = []
    dias_max = 30
    pagina = 1
    while True:
        page_url = base_url if pagina == 1 else f"{base_url}{pagina}/"
        print(f"Coletando p√°gina {pagina}: {page_url}")
        artigos = get_article_links_period(page_url, base_url, dias_max=dias_max)
        if not artigos:
            break  # se n√£o h√° artigos, encerra loop

        for titulo, link in artigos:
            try:
                artigo = get_article_content(link)
                artigo["pln"] = processar(artigo["texto"])  # NLP
                dados.append(artigo)
            except Exception as e:
                print("Erro no link", link, e)

        pagina += 1

    # Monta DataFrame com resultados
    df = pd.DataFrame(dados)
    df_tokens = pd.DataFrame({
        "titulo": df["titulo"],
        "manchete": df["manchete"],
        "autor": df["autor"],
        "data": df["data"],
        "link": df["link"],
        "texto_completo": df["texto"],
        "tokens": df["pln"].apply(lambda x: x["tokens"]),
        "stems": df["pln"].apply(lambda x: x["stems"]),
        "lemmas": df["pln"].apply(lambda x: x["lemmas"]),
    })

    print(df_tokens.head())

    # Se tiver news_service, salva no banco
    if news_service:
        print("üíæ Saving data to database...")
        saved_count = 0
        for _, row in df_tokens.iterrows():
            try:
                # Dados b√°sicos da not√≠cia
                news_data = {
                    'url': row['link'],
                    'data_publicacao': row['data'] if isinstance(row['data'], datetime) else None,
                    'autor': row['autor'],
                    'tipo_fonte': 'EXAME',
                    'categoria': None,
                    'sentimento': None,
                    'relevancia': 0.0
                }

                # Processamento textual (t√≠tulo, manchete e corpo)
                titulo_processed = processar(row['titulo'])
                manchete_processed = processar(row['manchete'])
                corpo_processed = processar(row['texto_completo'])

                text_processing_data = {
                    'titulo': {
                        'texto_bruto': row['titulo'],
                        'tokens_normalizados': ' '.join(titulo_processed['tokens']) if titulo_processed[
                            'tokens'] else None,
                        'tokens_stemming': ' '.join(titulo_processed['stems']) if titulo_processed['stems'] else None,
                        'tokens_lemma': ' '.join(titulo_processed['lemmas']) if titulo_processed['lemmas'] else None,
                    },
                    'manchete': {
                        'texto_bruto': row['manchete'],
                        'tokens_normalizados': ' '.join(manchete_processed['tokens']) if manchete_processed[
                            'tokens'] else None,
                        'tokens_stemming': ' '.join(manchete_processed['stems']) if manchete_processed[
                            'stems'] else None,
                        'tokens_lemma': ' '.join(manchete_processed['lemmas']) if manchete_processed[
                            'lemmas'] else None,
                    },
                    'corpo': {
                        'texto_bruto': row['texto_completo'],
                        'tokens_normalizados': ' '.join(corpo_processed['tokens']) if corpo_processed[
                            'tokens'] else None,
                        'tokens_stemming': ' '.join(corpo_processed['stems']) if corpo_processed['stems'] else None,
                        'tokens_lemma': ' '.join(corpo_processed['lemmas']) if corpo_processed['lemmas'] else None,
                    }
                }

                # Salva no banco via service externo
                ticker_id, news_id, processing_ids = news_service.save_complete_news_data_with_ticker_id(
                    ticker_id=ticker_id,
                    news_data=news_data,
                    text_processing_data=text_processing_data
                )

                if ticker_id and news_id and any(processing_ids):
                    saved_count += 1

            except Exception as e:
                print(f"Error saving news to database: {e}")
                continue

        print(f"‚úÖ Saved {saved_count} news articles to database")
    else:
        # Se n√£o houver banco, salva CSV
        filename = f"exame_{company_url}_ultimos_7_dias.csv"
        df_tokens.to_csv(filename, index=False, encoding="utf-8-sig")
        print(f"‚úÖ Conclu√≠do! Not√≠cias dos √∫ltimos 7 dias salvas em {filename}.")

    return True


# Execu√ß√£o direta (modo standalone)
if __name__ == "__main__":
    # Executa exemplo padr√£o para Banco do Brasil
    web_scrapping("BUY_&_HOLD", "Banco do Brasil")
